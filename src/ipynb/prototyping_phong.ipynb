{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T11:59:20.511570900Z",
     "start_time": "2024-07-17T11:59:20.504036300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Machine Learning and Data Analytics\\NLP Projekt\\Projektarbeit-NLP\\src\n",
      "C:\\Machine Learning and Data Analytics\\NLP Projekt\\Projektarbeit-NLP\\src\\ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phong\\.conda\\envs\\Natural Language Processing\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Ändern des Arbeitsverzeichnisses auf das übergeordnete Verzeichnis\n",
    "%cd ..\n",
    "# Jetzt können Sie das Modul relativ importieren\n",
    "from utils import data_import as di\n",
    "# Optional: Zurück zum ursprünglichen Verzeichnis wechseln\n",
    "%cd ipynb\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T11:59:23.893758700Z",
     "start_time": "2024-07-17T11:59:21.400956600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19100 recipes were imported.\n"
     ]
    }
   ],
   "source": [
    "TARGET_PATH_IMPORT = '../../data/raw_data/scraped_recipes_raw.json'\n",
    "\n",
    "dataset = pd.read_json(TARGET_PATH_IMPORT)\n",
    "print(f'{len(dataset.payload)} recipes were imported.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def show(p,n):\n",
    "    j = int(round(100 * p / n))\n",
    "    print(\"{}[{}{}] {}/{}\".format(\"%\", \"#\"*int(j), \".\"*(100-j), p, n),  end='\\r', file=sys.stdout, flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T11:59:25.394640400Z",
     "start_time": "2024-07-17T11:59:25.386153600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:05:02.474868800Z",
     "start_time": "2024-07-17T11:59:27.095992100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%[####################################################################################################] 19099/19100\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m     data_dict[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessed_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m preprocessed_tokens\n\u001B[0;32m     21\u001B[0m     show(i, \u001B[38;5;28mlen\u001B[39m(data_dict))\n\u001B[1;32m---> 23\u001B[0m display(data_dict\u001B[38;5;241m.\u001B[39mhead())\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "#Tokenisierung\n",
    " \n",
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(TARGET_PATH_IMPORT, 'r', encoding='utf-8') as file:\n",
    "    data_dict = json.load(file)\n",
    "\n",
    "data_dict = data_dict['payload']\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    preprocessed_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_digit and token.is_alpha]\n",
    "    return preprocessed_tokens\n",
    "\n",
    "for i in range(0, len(data_dict)):\n",
    "    buffer_text = data_dict[i]['preparation']\n",
    "    preprocessed_tokens = tokenize(buffer_text)\n",
    "    data_dict[i]['preprocessed_tokens'] = preprocessed_tokens\n",
    "    show(i, len(data_dict))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "with open('../../data/raw_data/scraped_recipes_raw_tokenized.json', 'w',encoding='utf-8') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T12:06:14.556022800Z",
     "start_time": "2024-07-17T12:06:11.171531500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:40:55.428660700Z",
     "start_time": "2024-07-16T21:40:55.406150300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/raw_data/scraped_recipes_raw_tokenized.json' 'r', encoding='utf-8') as file:\n",
    "    data_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T13:06:41.081162600Z",
     "start_time": "2024-07-16T13:06:39.807180600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
